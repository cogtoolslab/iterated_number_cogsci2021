{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "import os\n",
    "import urllib\n",
    "from io import BytesIO     # for handling byte strings\n",
    "from io import StringIO    # for handling unicode strings\n",
    "import pymongo as pm\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.spatial import distance\n",
    "from scipy.spatial.distance import cdist, pdist, squareform\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from joblib import dump, load\n",
    "from PIL import Image\n",
    "import base64\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['pdf.fonttype'] = 42\n",
    "# mpl.rcParams['pdf.fonttype'] = 42\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.lines as mlines\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "from sklearn import linear_model, datasets, neighbors, svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import clear_output\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "feature_dir = os.path.abspath(os.path.join(proj_dir,'features'))\n",
    "svg_dir = os.path.abspath(os.path.join(sketch_dir,'svg'))\n",
    "png_dir = os.path.abspath(os.path.join(sketch_dir,'png'))\n",
    "datastructures_dir = os.path.join(analysis_dir,'datastructures')\n",
    "\n",
    "## feature dirs\n",
    "#example_dir = os.path.abspath(os.path.join(feature_dir,'example'))\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'analysis') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'python'))\n",
    "    \n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.exists(plot_dir):\n",
    "    os.makedirs(plot_dir)   \n",
    "    \n",
    "if not os.path.exists(csv_dir):\n",
    "    os.makedirs(csv_dir)       \n",
    "\n",
    "if not os.path.exists(datastructures_dir):\n",
    "    os.makedirs(datastructures_dir)       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get only the features from one layer of the NN, FC6\n",
    "FEAT = np.load(os.path.join(feature_dir, \"FEATURES_FC6_sketch_channel-norm.npy\"))\n",
    "num_feats = np.shape(FEAT)[1] # the first dimension is number of sketches, second is number of features     \n",
    "FEAT = pd.DataFrame(FEAT)\n",
    "feat_cols = [str(i) for i in np.arange(num_feats)]\n",
    "FEAT.columns = feat_cols\n",
    "# FEAT.columns = FEAT.columns.astype(int)\n",
    "\n",
    "META = pd.read_csv(os.path.join(feature_dir,'METADATA_sketch.csv'))\n",
    "assert META.shape[0]==FEAT.shape[0]\n",
    "META['game_id'] = META.sketch_id.str.split('_').str[0]\n",
    "META['animal'] = META.sketch_id.str.split('_').str[1]\n",
    "META['cardinality'] = META.sketch_id.str.split('_').str[2]\n",
    "META['trial_num'] = META.sketch_id.str.split('_').str[3]\n",
    "META['condition'] = META.sketch_id.str.split('_').str[4]\n",
    "META.drop(columns=['feature_ind'],inplace=True)\n",
    "\n",
    "D = pd.concat([META,FEAT],axis=1)\n",
    "D = D.astype({'trial_num': 'float'})\n",
    "D['block'] = np.ceil(D['trial_num']/8)\n",
    "D = D.astype({'block': 'float'})\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(df, \n",
    "               folds=5, \n",
    "               random_seed=132,\n",
    "               replace=False,\n",
    "               group='animal', # or 'cardinality'; might want to create an e.g. 'rabbit_4'; can just \n",
    "               identifier='sketch_id'):\n",
    "    \n",
    "    num_obs_per_group = int(df.groupby(group).size().mean())\n",
    "    size = int(num_obs_per_group / folds)## how many obs do include in each split    \n",
    "    replace = False  # without replacement\n",
    "    ## create splits\n",
    "    splits = []\n",
    "    counter = 0\n",
    "    while counter < folds:\n",
    "        fn = lambda obj: obj.loc[np.random.RandomState(random_seed).choice(obj.index, size, replace),:]    \n",
    "        current_split = df.groupby(group, as_index=False).apply(fn)\n",
    "        used_ids = current_split.sketch_id.unique()\n",
    "        \n",
    "        df = df[~df.sketch_id.isin(used_ids)]\n",
    "        \n",
    "        ## sanity check, there is no overlap in image_id\n",
    "        assert len(np.intersect1d(current_split[identifier],df[identifier]))==0\n",
    "        splits.append(current_split.reset_index(drop=True))\n",
    "        counter += 1\n",
    "        \n",
    "    splits[-1] = splits[-1].append(df)\n",
    "    \n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_scoring(clfCond,gameCond):\n",
    "    DF = D[D['condition'] == gameCond] # number or shape\n",
    "    splits = get_splits(DF,group=clfCond) # cardinality or animal\n",
    "    \n",
    "    acc = []\n",
    "    clf_score_name = 'clfScore_' + clfCond\n",
    "    merging_df = pd.DataFrame(columns = ['sketch_id',clf_score_name])\n",
    "    \n",
    "    for ind,df in enumerate(splits):\n",
    "        training_dfs = splits[:ind] + splits[ind+1:]\n",
    "        trainset = pd.concat(training_dfs)\n",
    "        testset = df\n",
    "\n",
    "        Xtrain, Xtest = np.array(trainset[feat_cols]), np.array(testset[feat_cols])\n",
    "        ytrain, ytest = trainset[clfCond].values, testset[clfCond].values\n",
    "        \n",
    "        clf = linear_model.LogisticRegression(penalty='l2',\n",
    "                                          C=1e-3,\n",
    "                                          random_state=0,\n",
    "                                          solver='lbfgs',\n",
    "                                          multi_class='multinomial',\n",
    "                                          max_iter=1500)\n",
    "        clf.fit(Xtrain,ytrain)\n",
    "        score = clf.score(Xtest, ytest)\n",
    "        acc.append(score)\n",
    "        predictions = clf.predict(Xtest)\n",
    "        split_confmat = confusion_matrix(ytest, predictions)\n",
    "        \n",
    "        \n",
    "        df[clf_score_name] = predictions\n",
    "        addendum_for_merging = df[['sketch_id',clf_score_name]]\n",
    "        merging_df = merging_df.append(addendum_for_merging)\n",
    "        \n",
    "        \n",
    "        confmat = split_confmat if ind == 0 else split_confmat + confmat\n",
    "    \n",
    "#     DF = DF.merge(merging_df,on='sketch_id')\n",
    "#     print(len(DF[clf_score_name]))\n",
    "    return [acc,confmat,merging_df]\n",
    "\n",
    "# A=animal,C=cardinality ; S=shape,N=number\n",
    "ASacc,ASconf,ASdf = clf_scoring('animal','shape')\n",
    "ANacc,ANconf,ANdf = clf_scoring('animal','number')\n",
    "CSacc,CSconf,CSdf = clf_scoring('cardinality','shape')\n",
    "CNacc,CNconf,CNdf = clf_scoring('cardinality','number')\n",
    "\n",
    "# put together the full dataframe\n",
    "mergeDFanimal = ASdf.append(ANdf)\n",
    "mergeDFcardinality = CSdf.append(CNdf)\n",
    "D = D.merge(mergeDFanimal,on=\"sketch_id\")\n",
    "D = D.merge(mergeDFcardinality,on=\"sketch_id\")\n",
    "# now save it out!\n",
    "# np.save(os.path.join(datastructures_dir,\"clfD.npy\"), D)\n",
    "\n",
    "saveD = D.copy()\n",
    "saveD.drop(D.columns[6:4102], axis=1, inplace=True) # these just happen to be the feature columns, which we don't need\n",
    "saveD.to_csv(os.path.join(datastructures_dir,\"clfD.csv\"))\n",
    "\n",
    "# save out the confmats to the directory so we can import them into our recog analysis\n",
    "np.save(os.path.join(datastructures_dir,\"AA_CLFconfmat.npy\"), ASconf)\n",
    "np.save(os.path.join(datastructures_dir,\"CA_CLFconfmat.npy\"), ANconf) #number production, animal recog\n",
    "np.save(os.path.join(datastructures_dir,\"AC_CLFconfmat.npy\"), CSconf) #shape production, cardinality recog\n",
    "np.save(os.path.join(datastructures_dir,\"CC_CLFconfmat.npy\"), CNconf)\n",
    "\n",
    "np.save(os.path.join(datastructures_dir,\"AA_CLFacc.npy\"), ASacc)\n",
    "np.save(os.path.join(datastructures_dir,\"CA_CLFacc.npy\"), ANacc) #number production, animal recog\n",
    "np.save(os.path.join(datastructures_dir,\"AC_CLFacc.npy\"), CSacc) #shape production, cardinality recog\n",
    "np.save(os.path.join(datastructures_dir,\"CC_CLFacc.npy\"), CNacc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf95(arr, z=1.96):\n",
    "    mean = np.mean(arr)\n",
    "    std = np.std(arr)\n",
    "    answer = z * std / np.sqrt(np.size(arr))\n",
    "    return answer\n",
    "\n",
    "fig , (ax1,ax2) = plt.subplots(1,2 , sharey=True, figsize = (4,6))\n",
    "\n",
    "ax1.set_ylim(0,1)\n",
    "\n",
    "fig.suptitle(\"Model\",fontsize=24)\n",
    "\n",
    "# this subplot is for predicting shape\n",
    "ax1.bar([\"CA\",\"AA\"],\n",
    "        [np.mean(ANacc), # predicting animals based on numbers\n",
    "         np.mean(ASacc)], # predicting animals based on animals\n",
    "        color = ['#7DCCF4','#BD83C7'], #['#6a6e9c','#b53819'],\n",
    "        yerr = [cf95(ANacc),\n",
    "                cf95(ASacc)], error_kw={'linewidth':1.2,'capsize':4})\n",
    "\n",
    "# this subplot is for predicting number\n",
    "ax2.bar([\"CC\",\"AC\"],\n",
    "        [np.mean(CNacc),\n",
    "         np.mean(CSacc)],\n",
    "        color = ['#7DCCF4','#BD83C7'], #['#6a6e9c','#b53819'],\n",
    "        yerr = [cf95(CNacc),\n",
    "                cf95(CSacc)], error_kw={'linewidth':1.2,'capsize':4})\n",
    "\n",
    "l1=ax1.axhline(0.25,color='black',ls='--')\n",
    "l1.set_label('l1')\n",
    "\n",
    "l2=ax2.axhline(0.125,color='black',ls='--')\n",
    "l2.set_label('l2')\n",
    "\n",
    "ax1.set_xlabel(\"Animals\",color='#A04EAE') #b53819\n",
    "ax1.set_xticklabels([])\n",
    "ax1.set_yticks([0,.25,.5,.75,1])\n",
    "ax1.set_yticklabels(['0','.25','.5','.75','1'])\n",
    "ax1.set_ylabel(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Cardinalities\",color='#6369AF') #6a6e9c\n",
    "ax2.set_xticklabels([])\n",
    "\n",
    "# ax1.set_facecolor('#f9d9ff')\n",
    "# ax2.set_facecolor('#bfcbff')\n",
    "\n",
    "fig.text(0.5, 0, 'Predicting', ha='center', fontsize=18)\n",
    "# fig.tight_layout()\n",
    "\n",
    "# fig.subplots_adjust(bottom=0.15)\n",
    "\n",
    "fig.savefig('../results/plots/accModel.pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Congruent classifications:\")\n",
    "print(\"Cardinality games, number ratings mean: \",np.round(np.mean(CNacc),3), \n",
    "      \"\\n upper 95CI:\", np.round(np.mean(CNacc) + cf95(CNacc),3),\n",
    "      \"\\n lower 95CI: \",np.round(np.mean(CNacc) - cf95(CNacc),3))\n",
    "print(\"Animal games, shape ratings mean: \",np.round(np.mean(ASacc),3), \n",
    "      \"\\n upper 95CI:\", np.round(np.mean(ASacc) + cf95(ASacc),3),\n",
    "      \"\\n lower 95CI: \",np.round(np.mean(ASacc) - cf95(ASacc),3),'\\n\\n')\n",
    "\n",
    "print(\"Incongruent:\")\n",
    "print(\"Animal games, number ratings mean: \",np.round(np.mean(ANacc),3), \n",
    "      \"\\n upper 95CI:\", np.round(np.mean(ANacc) + cf95(ANacc),3),\n",
    "      \"\\n lower 95CI: \",np.round(np.mean(ANacc) - cf95(ANacc),3))\n",
    "print(\"Cardinality games, shape ratings mean: \",np.round(np.mean(CSacc),3), \n",
    "      \"\\n upper 95CI:\", np.round(np.mean(CSacc) + cf95(CSacc),3),\n",
    "      \"\\n lower 95CI: \",np.round(np.mean(CSacc) - cf95(CSacc),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
