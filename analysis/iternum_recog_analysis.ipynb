{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook is for analysing the results of the recognition task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import pymongo as pm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab, mlab, pyplot\n",
    "import matplotlib.patches as mpatches\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize, getfigs\n",
    "plt = pyplot\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "sns.set_style('white')\n",
    "from matplotlib import rcParams\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import importlib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# so dataframes don't get cut off in display:\n",
    "#pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "\n",
    "# a way to hide the little red error warnings that show up sometimes: (https://stackoverflow.com/questions/9031783/hide-all-warnings-in-ipython)    \n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show_err=false; \n",
    "function code_toggle_err() {\n",
    " if (code_show_err){\n",
    " $('div.output_stderr').hide();\n",
    " } else {\n",
    " $('div.output_stderr').show();\n",
    " }\n",
    " code_show_err = !code_show_err\n",
    "} \n",
    "$( document ).ready(code_toggle_err);\n",
    "</script>\n",
    "To toggle on/off output_stderr, click <a href=\"javascript:code_toggle_err()\">here</a>.''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory & file hierarchy\n",
    "proj_dir = os.path.abspath('..')\n",
    "analysis_dir = os.getcwd()\n",
    "results_dir = os.path.join(proj_dir,'results')\n",
    "plot_dir = os.path.join(results_dir,'plots')\n",
    "csv_dir = os.path.join(results_dir,'csv')\n",
    "exp_dir = os.path.abspath(os.path.join(proj_dir,'experiments'))\n",
    "datastructures_dir = os.path.join(analysis_dir,'datastructures')\n",
    "\n",
    "## add helpers to python path\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))   \n",
    "\n",
    "def make_dir_if_not_exists(dir_name):   \n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    return dir_name\n",
    "\n",
    "## create directories that don't already exist        \n",
    "result = [make_dir_if_not_exists(x) for x in [results_dir,plot_dir,csv_dir,datastructures_dir]]\n",
    "\n",
    "## add utils to python path\n",
    "import sys\n",
    "if os.path.join(proj_dir,'utils') not in sys.path:\n",
    "    sys.path.append(os.path.join(proj_dir,'utils'))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### establish connection to mongo\n",
    "first thing you need to do is to establish an ssh tunnel (aka remote port forwarding) to the server, so that requests to the mongodb can be made \"as if\" the mongodb server is running on your local computer. Run this from the command line before you begin data analysis if you plan to fetch data from mongo:\n",
    "\n",
    "ssh -fNL 27020:127.0.0.1:27017 USER@cogtoolslab.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ssh -fNL 27020:127.0.0.1:27017 sholt@cogtoolslab.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv(os.path.join(analysis_dir,'auth.txt'), header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "# have to fix this to be able to analyze from local\n",
    "import pymongo as pm\n",
    "import socket\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27017')\n",
    "db = conn['iterated_number']\n",
    "coll = db['num8_shape4_recognition']\n",
    "\n",
    "# which iteration name should we look into?\n",
    "iterationName = 'run1' \n",
    "# existing iterationNames: 'testing1', 'run1'\n",
    "\n",
    "# db.list_collection_names()\n",
    "# len(list(coll.find({'workerID':'XXXXXX'})))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## here is what one of these records looks like\n",
    "# coll.find_one()\n",
    "# There have been a total of 49 distinct workers, who produced 3729 total sketch ratings\n",
    "\n",
    "total_ratings = len(list(coll.find({'iterationName':'run1'})))\n",
    "distinct_workers = list(coll.find({'iterationName':'run1'}).distinct(\"workerID\"))\n",
    "\n",
    "print(\"There have been a total of {} distinct workers, who produced {} total sketch ratings\".format(len(distinct_workers),total_ratings))    \n",
    "\n",
    "for worker in distinct_workers:\n",
    "    print(worker, list(coll.find({'workerID':worker}).distinct(\"recog_gameID\")))\n",
    "\n",
    "\n",
    "# save the list of distinct recognizers out to a datastructure\n",
    "np.save(os.path.join(datastructures_dir,\"recognizers_list.npy\"),distinct_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fetch records that match our list of iterationNames\n",
    "print('Currently trying to generate  dataframe...')\n",
    "r = coll.find({'iterationName': iterationName})\n",
    "## create (raw, unfiltered) dataframe \n",
    "## data from iterationNames of interest\n",
    "R = pd.DataFrame(list(r))\n",
    "print('Finished generating dataframe.')\n",
    "\n",
    "## get list of valid game IDs (i.e, subject number)\n",
    "from collections import Counter\n",
    "game_dict = Counter(R['recog_gameID']) ## get dictionary mapping gameIDs to number of sketches \n",
    "complete_gameids = [r for (r,v) in game_dict.items() if v==65] ## get gameids that contributed exactly the right number of sketches\n",
    "\n",
    "## subset stroke/sketch dataframes by being complete\n",
    "subset = True\n",
    "if (subset and R['recog_gameID'].nunique()!=len(complete_gameids)):\n",
    "    R = R[(R['recog_gameID'].isin(complete_gameids))].reset_index(drop=True)\n",
    "    \n",
    "print('We have {} unique sketch records in all {} of our complete games.'.format(R.shape[0],len(complete_gameids)))\n",
    "\n",
    "# save out to csv\n",
    "R.to_csv(os.path.join(csv_dir,'iternum_recog_data.csv'),index=False)\n",
    "print('Successfully saved out our recog data CSV to {}.'.format(csv_dir))\n",
    "\n",
    "print()\n",
    "for i in game_dict.keys():\n",
    "    if game_dict[i] < 65:\n",
    "        print(i, game_dict[i])\n",
    "\n",
    "# We have 3510 unique sketch records in all 54 of our complete games.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Turning things that can always be numeric into numeric things\n",
    "R = R.astype({'orig_sketch_cardinality': 'float',\n",
    "              'rating_trial_num': 'float'})\n",
    "\n",
    "# add a new column for whether recognizer got the answer right\n",
    "R['correct'] = np.nan\n",
    "\n",
    "# now populate that column with correctness vals. We have to look at different columns for each rating condition\n",
    "for r_num in range(len(R['rating'])):\n",
    "    if R['rate_condition'][r_num] == 'number':\n",
    "        R['rating'][r_num] = float(R['rating'][r_num])\n",
    "        R['correct'][r_num] = R['rating'][r_num] == R['orig_sketch_cardinality'][r_num]\n",
    "    elif R['rate_condition'][r_num] == 'shape':\n",
    "        R['correct'][r_num] = R['rating'][r_num] == R['orig_sketch_animal'][r_num]\n",
    "            \n",
    "R = R.astype({'correct': 'float'})\n",
    "\n",
    "print(\"Total of {} unique workers participated\".format(len(R.workerID.unique())))\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Who passed the 4 catch trials? Only look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dataframe showing each recognizer's catch trial accuracy\n",
    "catches = R[R['catch_trial'] == True]\n",
    "catch_acc = pd.DataFrame(catches.groupby(['recog_gameID','workerID'])['correct'].mean())\n",
    "catch_acc = catch_acc.rename(columns={\"correct\": \"catch_accuracy\"})\n",
    "num_games = len(catch_acc)\n",
    "\n",
    "\n",
    "# now get a list of the gameIDs who got perfect score on catch trials and filter everyone else out of the big DF\n",
    "kosher_games = list(catch_acc[catch_acc['catch_accuracy'] == 1.0].reset_index()['recog_gameID'])\n",
    "boolean_series = R.recog_gameID.isin(kosher_games) # get the indices of games that pass\n",
    "D = R[boolean_series] # now filter R by those games, and call it D\n",
    "D = D[D['catch_trial'] == False] # now remove all catch trials from D, as we don't need to analyse them\n",
    "num_kosher_games = len(kosher_games)\n",
    "\n",
    "# catch_acc # here's that dataframe\n",
    "\n",
    "print(\"{} recog games passed, out of {} total.\".format(num_kosher_games,num_games))\n",
    "print(\"{} total unique workers passed this stage\".format(len(D.workerID.unique())))\n",
    "# The following line is for analyze_survey.ipynb so we can get all the worker IDs that are valid\n",
    "# (forgot to save game_id to MTurk, but worker_id is also unique to game):\n",
    "# print(list(catch_acc[catch_acc['catch_accuracy'] == 1.0].reset_index()['workerID']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now also exclude any workers who participated in the original task (for some reason, store.js wasn't doing this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the workers who participated in the production task\n",
    "workerList = np.load(\"datastructures/workers_list.npy\")\n",
    "boolean_series = D.workerID.isin(workerList)\n",
    "D = D[~boolean_series]\n",
    "num_naive = len(D.workerID.unique())\n",
    "num_games_naive = len(D.recog_gameID.unique())\n",
    "\n",
    "print(\"{} recog participants not repeated from production.\".format(num_naive))\n",
    "print(\"{} recog games not from repeaters, out of {} total.\".format(num_games_naive,num_kosher_games))\n",
    "print(\"{} total unique workers passed this stage\".format(len(D.workerID.unique())))\n",
    "\n",
    "\n",
    "# save this list of distinct recognizers out to a datastructure\n",
    "np.save(os.path.join(datastructures_dir,\"valid_recognizers_list.npy\"),D.workerID.unique())\n",
    "\n",
    "# now save out the whole dataframe so we can run analyses in R:\n",
    "D.to_csv(os.path.join(datastructures_dir,\"recogData.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now count how many times each sketch has been rated by a valid, naÃ¯ve recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test = pd.DataFrame(D.groupby(['orig_game_id','orig_sketch_animal','orig_sketch_cardinality','rate_condition'])['orig_game_trial_num'].agg(['count'])).reset_index()            \n",
    "# gameIDs = pd.unique(test['orig_game_id'])\n",
    "\n",
    "# for game in gameIDs:\n",
    "#     plt.figure(figsize=(4,16))\n",
    "#     test[test['orig_game_id'] == game][test['rate_condition'] == 'shape'].reset_index().sort_values(by='count').plot.bar(y='count',x='index')#['count'].min()\n",
    "#     plt.xlabel('Stimulus Partition')\n",
    "#     plt.ylabel('# of Recog Games (shape)')\n",
    "    \n",
    "#     plt.figure(figsize=(4,16))\n",
    "#     test[test['orig_game_id'] == game][test['rate_condition'] == 'number'].reset_index().sort_values(by='count').plot.bar(y='count',x='index')#['count'].min()\n",
    "#     plt.xlabel('Stimulus Partition')\n",
    "#     plt.ylabel('# of Recog Games (number)')\n",
    "\n",
    "# test[test['orig_game_id'] == '0074-988d4ee1-5766-47b5-bcbb-49a720aee30d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_game_ids = np.unique(D[['recog_gameID']])\n",
    "\n",
    "# save the list of valid gameIDs out to a datastructure so we can do the patch for getting missed recogs\n",
    "np.save(os.path.join(datastructures_dir,\"valid_game_ids.npy\"),valid_game_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counting = pd.DataFrame(D.groupby(['orig_game_id','orig_game_trial_num','rate_condition'])['orig_game_trial_num'].agg(['count'])).reset_index()    \n",
    "# D.columns\n",
    "counting['count']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General accuracy measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mean accuracty for each\n",
    "\n",
    "def get_confint(df):\n",
    "    \"\"\"Takes a df already with columns ['Factors','Mean','Count','STD'] \"\"\"\n",
    "    new_df = df\n",
    "    ci95_hi = []\n",
    "    ci95_lo = []\n",
    "    for i in new_df.index:\n",
    "        m, c, s = new_df.loc[i]\n",
    "        ci95_hi.append(m + 1.96*s/math.sqrt(c))\n",
    "        ci95_lo.append(m - 1.96*s/math.sqrt(c))\n",
    "    new_df['ci95_hi'] = ci95_hi\n",
    "    new_df['ci95_lo'] = ci95_lo\n",
    "    return new_df\n",
    "\n",
    "condition_acc = get_confint(pd.DataFrame(D.groupby(['rate_condition','orig_game_condition'])['correct'].agg(['mean', 'count', 'std']))).reset_index()        \n",
    "recognizer_acc = D.groupby(['rate_condition','orig_game_condition','workerID'])['correct'].mean().reset_index()    \n",
    "CC_recognizer_acc = np.array(recognizer_acc[(recognizer_acc['orig_game_condition'] == 'number') & (recognizer_acc['rate_condition'] == 'number')]['correct'])        \n",
    "AC_recognizer_acc = np.array(recognizer_acc[(recognizer_acc['orig_game_condition'] == 'shape') & (recognizer_acc['rate_condition'] == 'number')]['correct'])        \n",
    "CA_recognizer_acc = np.array(recognizer_acc[(recognizer_acc['orig_game_condition'] == 'number') & (recognizer_acc['rate_condition'] == 'shape')]['correct'])        \n",
    "AA_recognizer_acc = np.array(recognizer_acc[(recognizer_acc['orig_game_condition'] == 'shape') & (recognizer_acc['rate_condition'] == 'shape')]['correct'])        \n",
    "\n",
    "\n",
    "# condition_acc # here's that dataframe\n",
    "\n",
    "\n",
    "ax1 = condition_acc.plot.bar(y='mean',\n",
    "                             yerr=condition_acc['ci95_hi'] - condition_acc['mean'],\n",
    "                             rot=30,\n",
    "                             legend=False) # here's a plot\n",
    "ax1.set_title(\"Recog Accuracy by Conditions\")\n",
    "ax1.get_children()[1].set_color((0,0,0,0))\n",
    "ax1.get_children()[2].set_color((0,0,0,0))\n",
    "ax1.get_children()[3].set_color((0,0,0,0))\n",
    "ax1.get_children()[4].set_color((0,0,0,0))\n",
    "\n",
    "ax1.get_children()[1].set_edgecolor('#7DCCF4')\n",
    "ax1.get_children()[2].set_edgecolor('#7DCCF4')\n",
    "ax1.get_children()[3].set_edgecolor('#BD83C7')\n",
    "ax1.get_children()[4].set_edgecolor('#BD83C7')\n",
    "\n",
    "ax1.scatter(0 + np.random.random(CC_recognizer_acc.size) * .03, CC_recognizer_acc, color='#7DCCF4',s=16,alpha=.6)\n",
    "ax1.scatter(1 + np.random.random(AC_recognizer_acc.size) * .03, AC_recognizer_acc, color='#7DCCF4',s=16,alpha=.6)\n",
    "\n",
    "ax1.scatter(2 + np.random.random(CA_recognizer_acc.size) * .03, CA_recognizer_acc, color='#BD83C7',s=16,alpha=.6)\n",
    "ax1.scatter(3 + np.random.random(AA_recognizer_acc.size) * .03, AA_recognizer_acc, color='#BD83C7',s=16,alpha=.6)\n",
    "\n",
    "ax1.set_xlabel(\"# Recog          Shape Recog\")\n",
    "ax1.set_xticklabels(['# Game','Shape Game','# Game','Shape Game'])\n",
    "# condition_acc.reset_index().columns\n",
    "# ax1.__dict__['_axes']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_acc#['mean'].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig , (ax1,ax2) = plt.subplots(1,2 , figsize = (4,3.75))\n",
    "\n",
    "ax1.set_ylim(0,1)\n",
    "ax2.set_ylim(0,1)\n",
    "\n",
    "# fig.suptitle(\"Human\", fontsize=24)\n",
    "\n",
    "shape_acc = condition_acc[condition_acc['rate_condition'] == 'shape']\n",
    "ax1.bar(shape_acc['orig_game_condition'],\n",
    "        height=shape_acc['mean'],\n",
    "        color = ['#7DCCF4','#BD83C7'], alpha=1,\n",
    "        yerr= shape_acc['ci95_hi']-shape_acc['mean'],\n",
    "        error_kw={'linewidth':1.2,'capsize':4})\n",
    "\n",
    "# ax1.set_xlabel(\"Animals\",color='#A04EAE')\n",
    "# ax1.set_xticklabels([])\n",
    "ax1.set_yticks([])\n",
    "ax1.set_yticklabels([])\n",
    "ax1.set_xticks([.5])\n",
    "ax1.set_xticklabels([\"Animals\"])\n",
    "\n",
    "number_acc = condition_acc[condition_acc['rate_condition'] == 'number']\n",
    "ax2.bar(number_acc['orig_game_condition'],\n",
    "        height=number_acc['mean'],\n",
    "        color = ['#7DCCF4','#BD83C7'], alpha=1,\n",
    "        yerr= number_acc['ci95_hi']-number_acc['mean'],\n",
    "        error_kw={'linewidth':1.2,'capsize':4})\n",
    "\n",
    "\n",
    "# ax2.set_xlabel(\"Cardinalities\",color='#6369AF')\n",
    "ax2.set_xticks([.5])\n",
    "ax2.set_xticklabels([\"Cardinalities\"])\n",
    "\n",
    "l1=ax1.axhline(0.25,color='black',ls='--')\n",
    "l1.set_label('l1')\n",
    "\n",
    "l2=ax2.axhline(0.125,color='black',ls='--')\n",
    "l2.set_label('l2')\n",
    "\n",
    "ax2.set_yticks([0,.25,.5,.75,1])\n",
    "ax2.set_yticklabels(['0','.25','.5','.75','1'])\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_label_position(\"right\")\n",
    "# ax2.set_ylabel(\"Accuracy\",rotation=270)#,position=(1.5,.5))\n",
    "ax2.tick_params(axis='y',  labelright='on')\n",
    "\n",
    "fig.text(0.5, .02, 'Predicting', ha='center', fontsize=18)\n",
    "\n",
    "\n",
    "# maybe include individual subjects' performance as well?\n",
    "# ax1.get_children()[1].set_color((0,0,0,0))\n",
    "# ax1.get_children()[2].set_color((0,0,0,0))\n",
    "# ax1.get_children()[1].set_edgecolor('#7DCCF4')\n",
    "# ax1.get_children()[2].set_edgecolor('#BD83C7')\n",
    "# ax2.get_children()[1].set_color((0,0,0,0))\n",
    "# ax2.get_children()[2].set_color((0,0,0,0))\n",
    "# ax2.get_children()[1].set_edgecolor('#7DCCF4')\n",
    "# ax2.get_children()[2].set_edgecolor('#BD83C7')\n",
    "\n",
    "# ax1.scatter(0 + np.random.random(CA_recognizer_acc.size) * .03, CA_recognizer_acc, color='k',s=4,alpha=1) #BD83C7\n",
    "# ax1.scatter(1 + np.random.random(AA_recognizer_acc.size) * .03, AA_recognizer_acc, color='k',s=4,alpha=1)\n",
    "# ax2.scatter(0 + np.random.random(CC_recognizer_acc.size) * .03, CC_recognizer_acc, color='k',s=4,alpha=1) #7DCCF4    \n",
    "# ax2.scatter(1 + np.random.random(AC_recognizer_acc.size) * .03, AC_recognizer_acc, color='k',s=4,alpha=1)\n",
    "\n",
    "# colors = ['#b53819', '#6a6e9c']\n",
    "# texts = [\"Animal Game Data\", \"Number Game Data\"]\n",
    "# patches = [ mpatches.Patch(color=colors[i], label=\"{:s}\".format(texts[i]) ) for i in range(len(texts)) ]\n",
    "# ax2.legend(handles=patches, bbox_to_anchor=(1.5, .9), ncol=1 )\n",
    "\n",
    "# you can't win. Theres no way:\n",
    "# https://stackoverflow.com/questions/35921626/legend-being-cut-off-on-saving-matplotlib\n",
    "# https://stackoverflow.com/questions/45239261/matplotlib-savefig-text-chopped-off\n",
    "# https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html\n",
    "# nothing works\n",
    "\n",
    "\n",
    "\n",
    "fig.subplots_adjust(left=.2, right=.8,top=.9,bottom=0.2)\n",
    "# plt.tight_layout()\n",
    "\n",
    "fig.savefig('../results/plots/accHuman.pdf')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the measures above over original trial number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# what should be on x axis? orig_game_trial_num; or orig_sketch_cardinality; or orig_sketch_animal\n",
    "IV = 'orig_sketch_cardinality'\n",
    "\n",
    "num_aml_trial_acc = get_confint(pd.DataFrame(D[(D['rate_condition'] == 'number') & (D['orig_game_condition'] == 'shape')].reset_index().groupby([IV])['correct'].agg(['mean', 'count', 'std'])))   \n",
    "num_num_trial_acc = get_confint(pd.DataFrame(D[(D['rate_condition'] == 'number') & (D['orig_game_condition'] == 'number')].reset_index().groupby([IV])['correct'].agg(['mean', 'count', 'std'])))   \n",
    "aml_aml_trial_acc = get_confint(pd.DataFrame(D[(D['rate_condition'] == 'shape') & (D['orig_game_condition'] == 'shape')].reset_index().groupby([IV])['correct'].agg(['mean', 'count', 'std'])))   \n",
    "aml_num_trial_acc = get_confint(pd.DataFrame(D[(D['rate_condition'] == 'shape') & (D['orig_game_condition'] == 'number')].reset_index().groupby([IV])['correct'].agg(['mean', 'count', 'std'])))   \n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(range(len(num_aml_trial_acc)),num_aml_trial_acc['mean'],\n",
    "             yerr=num_aml_trial_acc['ci95_hi']-num_aml_trial_acc['mean'],\n",
    "             color='#7DCCF4', linestyle='--') # rate number based on animal sketches\n",
    "plt.errorbar(range(len(num_num_trial_acc)),num_num_trial_acc['mean'],\n",
    "             yerr=num_num_trial_acc['ci95_hi']-num_num_trial_acc['mean'],\n",
    "             color='#7DCCF4')\n",
    "plt.errorbar(range(len(aml_aml_trial_acc)),aml_aml_trial_acc['mean'],\n",
    "             yerr=aml_aml_trial_acc['ci95_hi']-aml_aml_trial_acc['mean'],\n",
    "             color='#BD83C7')\n",
    "plt.errorbar(range(len(aml_num_trial_acc)),aml_num_trial_acc['mean'],\n",
    "             yerr=aml_num_trial_acc['ci95_hi']-aml_num_trial_acc['mean'],\n",
    "             color='#BD83C7',linestyle='--')\n",
    "\n",
    "plt.xlabel(IV)\n",
    "plt.ylabel(\"Recognition Accuracy\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recreate the same figures from VGG-19      (acc + confusion matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall, we want to centre the human recognition performance; the VGG is interesting to examine in light of this,\n",
    "# but the informativeness of the VGG results is dependent on their relation to the actual human performance\n",
    "\n",
    "numbers = [1,2,3,4,5,6,7,8]\n",
    "animals = ['bear','deer','owl','rabbit']\n",
    "\n",
    "def get_conf_mat(df,cond='shape'):\n",
    "    aml_map = {'bear':0,'deer':1,'owl':2,'rabbit':3}\n",
    "    num_map = {1:0,2:1,3:2,4:3,5:4,6:5,7:6,8:7}\n",
    "    if cond == 'number':\n",
    "        mat = np.zeros((len(num_map.keys()),len(num_map.keys())))\n",
    "        for row in df.itertuples():\n",
    "            orig_index = num_map[float(row.orig_sketch_cardinality)]\n",
    "            recog_index = num_map[float(row.rating)]\n",
    "            mat[orig_index,recog_index] += 1\n",
    "    else:\n",
    "        mat = np.zeros((len(aml_map.keys()),len(aml_map.keys())))\n",
    "        for row in df.itertuples():\n",
    "            orig_index = aml_map[row.orig_sketch_animal]\n",
    "            recog_index = aml_map[row.rating]\n",
    "            mat[orig_index,recog_index] += 1\n",
    "\n",
    "    return mat\n",
    "\n",
    "num_num_conf = get_conf_mat(D[(D['rate_condition'] == 'number') & (D['orig_game_condition'] == 'number')][['rating','orig_sketch_cardinality']], 'number')        \n",
    "num_aml_conf = get_conf_mat(D[(D['rate_condition'] == 'number') & (D['orig_game_condition'] == 'shape')][['rating','orig_sketch_cardinality']],'number')     \n",
    "aml_aml_conf = get_conf_mat(D[(D['rate_condition'] == 'shape') & (D['orig_game_condition'] == 'shape')][['rating','orig_sketch_animal']]) \n",
    "aml_num_conf = get_conf_mat(D[(D['rate_condition'] == 'shape') & (D['orig_game_condition'] == 'number')][['rating','orig_sketch_animal']])\n",
    "\n",
    "# get the confmats that we saved out from the CLF\n",
    "CLF_conf_AA = np.load(\"datastructures/AA_CLFconfmat.npy\") # guessing animal from number game\n",
    "CLF_conf_AC = np.load(\"datastructures/AC_CLFconfmat.npy\") # guessing number from animal game\n",
    "CLF_conf_CC = np.load(\"datastructures/CC_CLFconfmat.npy\")\n",
    "CLF_conf_CA = np.load(\"datastructures/CA_CLFconfmat.npy\")\n",
    "\n",
    "\n",
    "max_heat_val = np.max([np.max(num_num_conf),\n",
    "                       np.max(num_aml_conf),\n",
    "                       np.max(aml_aml_conf),\n",
    "                       np.max(aml_num_conf)])\n",
    "min_heat_val = np.min([np.min(num_num_conf),\n",
    "                       np.min(num_aml_conf),\n",
    "                       np.min(aml_aml_conf),\n",
    "                       np.min(aml_num_conf)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both the CLF and recog task confmats in the same plot\n",
    "all_confmats = [aml_aml_conf,\n",
    "                aml_num_conf, # recog number, production animal\n",
    "                num_num_conf,\n",
    "                num_aml_conf] # recog animal, production number\n",
    "\n",
    "## create lookup table of raw confmats (counts, not yet normalized)\n",
    "clf_list = ['aml_aml_conf','aml_num_conf','num_num_conf','num_aml_conf']\n",
    "clf_dict = dict(zip(clf_list,all_confmats))\n",
    "\n",
    "## init normed_confmats with zeros\n",
    "normed_confmats = dict()\n",
    "for this_clf in clf_list:\n",
    "    ## get dims of this confmat\n",
    "    these_dims = np.shape(clf_dict[this_clf])\n",
    "    normed_confmats[this_clf] = np.zeros(these_dims)\n",
    "\n",
    "# normalize all confmats by the sum of the rows (which we controlled to have equal numbers)\n",
    "for mat_ind, this_matname in enumerate(list(clf_dict.keys())):\n",
    "    this_mat = clf_dict[this_matname]\n",
    "    for row_ind, this_row in enumerate(this_mat):\n",
    "        normed_confmats[this_matname][row_ind,:] = this_row/np.sum(this_row)\n",
    "\n",
    "conf_fig, ((aml_aml_hm,aml_num_hm),(num_num_hm,num_aml_hm)) = plt.subplots(2,2, figsize=(10,10))\n",
    "\n",
    "all_axes = [aml_aml_hm,\n",
    "            aml_num_hm,\n",
    "            num_num_hm,\n",
    "            num_aml_hm]\n",
    "\n",
    "for subp_i in range(len(all_axes)):\n",
    "    subp = all_axes[subp_i]\n",
    "    confmat = normed_confmats[clf_list[subp_i]]\n",
    "    # some lines that will change the heatmap bounds to either be for all plots, or just plots of a given label set:\n",
    "    cbarval = False if subp_i in [1,3,5,7] else False\n",
    "    cmapval = 'YlGnBu' if subp_i > 1 else 'YlOrBr'\n",
    "    ax = sns.heatmap(confmat,cmap=cmapval,cbar=cbarval,ax=subp, vmin=0,vmax=1, square=True)\n",
    "\n",
    "\n",
    "\n",
    "# now plot everything as four heatmaps, one for each combination of recog and production condition\n",
    "# conf_fig, ((aml_aml,aml_num), (num_num,num_aml)) = plt.subplots(2,2, figsize=(10,10))\n",
    "conf_fig.suptitle(\"Confusion Matrices for Recognition Task\",fontsize=24)\n",
    "# aml_aml_hm = sns.heatmap(aml_aml_conf,cmap='YlOrBr',cbar=False,ax=aml_aml)#, vmin=min_heat_val,vmax=max_heat_val) # recog aml, prod aml\n",
    "# aml_num_hm = sns.heatmap(aml_num_conf,cmap=\"YlOrBr\",cbar=False,ax=aml_num)#, vmin=min_heat_val,vmax=max_heat_val) # recog aml, prod num\n",
    "\n",
    "# num_num_hm = sns.heatmap(num_num_conf,cmap=\"YlGnBu\",cbar=False,ax=num_num)#, vmin=min_heat_val,vmax=max_heat_val) # recog num, prod num\n",
    "# num_aml_hm = sns.heatmap(num_aml_conf,cmap=\"YlGnBu\",cbar=False,ax=num_aml)#, vmin=min_heat_val,vmax=max_heat_val) # recog num, prod aml    cmap='YlGnBu' ?\n",
    "\n",
    "aml_aml_hm.set_yticklabels(['bear','deer','owl','rabbit'],rotation=0)\n",
    "aml_aml_hm.set_xticklabels(['bear','deer','owl','rabbit'])\n",
    "aml_aml_hm.tick_params(labelbottom=False,labeltop=True)\n",
    "aml_num_hm.set_xticklabels(['bear','deer','owl','rabbit'])\n",
    "aml_num_hm.tick_params(labelbottom=False,labeltop=True)\n",
    "aml_num_hm.set_yticklabels([])\n",
    "\n",
    "num_num_hm.set_yticklabels(['1','2','3','4','5','6','7','8'],rotation=0)\n",
    "num_num_hm.set_xticklabels(['1','2','3','4','5','6','7','8'])\n",
    "num_aml_hm.set_xticklabels(['1','2','3','4','5','6','7','8'])\n",
    "num_aml_hm.set_yticklabels([])\n",
    "\n",
    "aml_num_hm.set_ylabel(\"Classifying Animal\",rotation=90)\n",
    "aml_num_hm.yaxis.set_label_position(\"right\")\n",
    "aml_num_hm.set_xlabel(\"Incongruent Games\")\n",
    "aml_num_hm.xaxis.set_label_position(\"top\")\n",
    "aml_aml_hm.set_xlabel(\"Congruent Games\")\n",
    "aml_aml_hm.xaxis.set_label_position(\"top\")\n",
    "\n",
    "num_aml_hm.set_ylabel(\"Classifying Number\",rotation=90)\n",
    "num_aml_hm.yaxis.set_label_position(\"right\")\n",
    "\n",
    "conf_fig.subplots_adjust(hspace=.1,wspace=.1)\n",
    "\n",
    "conf_fig.tight_layout()\n",
    "\n",
    "conf_fig.savefig('../results/plots/humanconfmats.pdf')\n",
    "\n",
    "# rows (y=axis) are the original sketch label (shape or number), columns (x-axis) are the recog assigned label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both the CLF and recog task confmats in the same plot\n",
    "all_confmats = [aml_aml_conf, CLF_conf_AA,\n",
    "                aml_num_conf, CLF_conf_CA, # recog number, production animal\n",
    "                num_num_conf, CLF_conf_CC,\n",
    "                num_aml_conf, CLF_conf_AC] # recog animal, production number\n",
    "\n",
    "## create lookup table of raw confmats (counts, not yet normalized)\n",
    "clf_list = ['aml_aml_conf', 'CLF_conf_AA','aml_num_conf', 'CLF_conf_CA',\n",
    "'num_num_conf', 'CLF_conf_CC','num_aml_conf', 'CLF_conf_AC']\n",
    "clf_dict = dict(zip(clf_list,all_confmats))\n",
    "\n",
    "## init normed_confmats with zeros\n",
    "normed_confmats = dict()\n",
    "for this_clf in clf_list:\n",
    "    ## get dims of this confmat\n",
    "    these_dims = np.shape(clf_dict[this_clf])\n",
    "    normed_confmats[this_clf] = np.zeros(these_dims)\n",
    "\n",
    "# normalize all confmats by the sum of the rows (which we controlled to have equal numbers)\n",
    "for mat_ind, this_matname in enumerate(list(clf_dict.keys())):\n",
    "    this_mat = clf_dict[this_matname]\n",
    "    for row_ind, this_row in enumerate(this_mat):\n",
    "        normed_confmats[this_matname][row_ind,:] = this_row/np.sum(this_row)\n",
    "\n",
    "conf_fig, ((axRecog1,axCLF1), (axRecog2,axCLF2), (axRecog3,axCLF3), (axRecog4,axCLF4)) = plt.subplots(4,2, figsize=(10,20))\n",
    "\n",
    "all_axes = [axRecog1, axCLF1,\n",
    "            axRecog2, axCLF2,\n",
    "            axRecog3, axCLF3,\n",
    "            axRecog4, axCLF4]\n",
    "\n",
    "for subp_i in range(len(all_axes)):\n",
    "    subp = all_axes[subp_i]\n",
    "    confmat = normed_confmats[clf_list[subp_i]]\n",
    "    # some lines that will change the heatmap bounds to either be for all plots, or just plots of a given label set:\n",
    "    cbarval = False if subp_i in [1,3,5,7] else False\n",
    "    cmapval = 'YlGnBu' if subp_i > 3 else 'YlOrBr'\n",
    "    ax = sns.heatmap(confmat,cmap=cmapval,cbar=cbarval,ax=subp, vmin=0,vmax=1, square=True)\n",
    "\n",
    "axRecog1.set_yticklabels(['bear','deer','owl','rabbit'],rotation=0,fontsize=20)\n",
    "axRecog1.set_xticklabels(['bear','deer','owl','rabbit'],fontsize=20)\n",
    "axRecog1.tick_params(labelbottom=False,labeltop=True)\n",
    "axCLF1.set_xticklabels(['bear','deer','owl','rabbit'],fontsize=20)\n",
    "axCLF1.tick_params(labelbottom=False,labeltop=True)\n",
    "axCLF1.set_yticklabels([])\n",
    "axRecog2.set_yticklabels(['bear','deer','owl','rabbit'],rotation=0,fontsize=20)\n",
    "axRecog2.set_xticklabels([])\n",
    "axCLF2.set_yticklabels([])\n",
    "axCLF2.set_xticklabels([])\n",
    "\n",
    "\n",
    "axRecog3.set_yticklabels(['1','2','3','4','5','6','7','8'],rotation=0,fontsize=20)\n",
    "axRecog3.set_xticklabels([])\n",
    "axCLF3.set_xticklabels([])\n",
    "axCLF3.set_yticklabels([])\n",
    "\n",
    "axRecog4.set_yticklabels(['1','2','3','4','5','6','7','8'],rotation=0,fontsize=20)\n",
    "axRecog4.set_xticklabels(['1','2','3','4','5','6','7','8'],fontsize=20)\n",
    "axCLF4.set_xticklabels(['1','2','3','4','5','6','7','8'],fontsize=20)\n",
    "axCLF4.set_yticklabels([])\n",
    "\n",
    "# setting labels on the top\n",
    "axRecog1.set_xlabel(\"Human Confusions\",fontsize=24) # 'human' recognition, interpretation, or classification\n",
    "axRecog1.xaxis.set_label_position(\"top\")\n",
    "axCLF1.set_xlabel(\"Model Confusions\",fontsize=24) # 'model' recognition, interpretation, or classification\n",
    "axCLF1.xaxis.set_label_position(\"top\")\n",
    "\n",
    "# setting labels down the right side\n",
    "axCLF1.set_ylabel(\"Animal Games, Animal Ratings\",rotation=90)\n",
    "axCLF1.yaxis.set_label_position(\"right\")\n",
    "axCLF2.set_ylabel(\"Number Games, Animal Ratings\",rotation=90)\n",
    "axCLF2.yaxis.set_label_position(\"right\")\n",
    "axCLF3.set_ylabel(\"Number Games, Number Ratings\",rotation=90)\n",
    "axCLF3.yaxis.set_label_position(\"right\")\n",
    "axCLF4.set_ylabel(\"Animal Games, Number Ratings\",rotation=90)\n",
    "axCLF4.yaxis.set_label_position(\"right\")\n",
    "\n",
    "\n",
    "conf_fig.tight_layout()\n",
    "conf_fig.subplots_adjust(hspace=.05,wspace=.05)\n",
    "\n",
    "conf_fig.savefig('../results/plots/confmats.pdf')\n",
    "\n",
    "# cb_ax = conf_fig.add_axes([0.83, 0.1, 0.02, 0.8])\n",
    "# cbar = conf_fig.colorbar(ax, cax=cb_ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-hoc num_aml_conf analysis to see heterogeneity in confmat\n",
    "tosses = np.sum(num_aml_conf , axis = 1) # the number of each cardinality given\n",
    "guesses = np.sum( num_aml_conf , axis = 0) # this gets the total number of guesses of each label\n",
    "guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.arange(len(guesses)),guesses)\n",
    "m, b = np.polyfit(np.arange(len(guesses)), guesses, 1)\n",
    "plt.plot(np.arange(len(guesses)), m*np.arange(len(guesses)) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare\n",
    "np.round(chisquare(guesses,f_exp=tosses),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post-hoc num_aml_conf analysis to see heterogeneity in confmat\n",
    "tosses = np.sum(aml_num_conf , axis = 1) # the number of each animal given\n",
    "guesses = np.sum( aml_num_conf , axis = 0) # this gets the total number of guesses of each label\n",
    "guesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(np.arange(len(guesses)),guesses)\n",
    "m, b = np.polyfit(np.arange(len(guesses)), guesses, 1)\n",
    "plt.plot(np.arange(len(guesses)), m*np.arange(len(guesses)) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(chisquare(guesses,f_exp=tosses),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the effect of cardinality on recog RT\n",
    "get_confint(D.groupby(['orig_sketch_cardinality'])['RT'].agg(['mean','count','std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(all_confmats[0].flatten(),all_confmats[1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.spearmanr(all_confmats[4].flatten(),all_confmats[5].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(all_confmats[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Methods:\n",
    "# 1. repeated reference game\n",
    "    # Procedure\n",
    "    # Stimuli etc.\n",
    "    \n",
    "# 2. recog task\n",
    "    # # Procedure\n",
    "    # Stimuli etc.\n",
    "    \n",
    "# 3. model-based analyses of the drawings\n",
    "    # describe CNN structure, training, validation\n",
    "    \n",
    "\n",
    "# Results:\n",
    "# sequence of take-home points\n",
    "# 1) people are indeed able to differentially communicate number vs shape (acc, RT) without existing symbols\n",
    "    # create a figure showing accuracy over time\n",
    "# 2) we notice that stroke number correlates with cardinality when relevant (ink less so), but not otherwise   \n",
    "\n",
    "# 3) numeral symbols (ones from number game) were TRANSPARENT to strangers (this is a more socially valid measure)\n",
    "    # ask if recog task number recognition is highest for those number sketches that use the num_strokes correlation with cardinality     \n",
    "    # stats: 1) overall accuracy relative to chance; 2) accuracy relative to context; 3) does numstrokes number recog acc on num_games?    \n",
    "\n",
    "# 4) VGG trained on corpus of mostly objects (Imagenet) differential performance on shape vs number.\n",
    "# How sensitive is it to exact number?      \n",
    "# \"Vision model trained on shape discrimination is good at shape discrimination, but only weakly sensitive to number information\"\n",
    "# comment on the finding that therefore number discrimination isn't a very natural extension of other visual features\n",
    "# category discriminating pre-training objective seems to reach a limit at numerosity! What other mechanisms do we need?    \n",
    "# to what degree do the VGG representations look like what's thought to go on in humans when they work with number\n",
    "# Look at human confusion matrices. Look at VGG confusion matrices. Are they similar?\n",
    "# They might be more similar for shape than for number\n",
    "\n",
    "# look at Spearman correlation between conf_mats (turned into 1-D vectors) of humans and VGG\n",
    "# put it onto a scatterplot. That correlation might be alright for shape, probably won't for number\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Qualitative, impressionistic: the kinds of strategies that people seem to be using (?)\n",
    "# classifiers, \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupy factor: iterationName, rate_condition, orig_game_condition, workerID\n",
    "all_acc = get_confint(pd.DataFrame(D.groupby(['rate_condition'])['correct'].agg(['mean', 'count', 'std']))).reset_index()        \n",
    "all_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognizer_acc = pd.DataFrame(D.groupby(['rate_condition','orig_game_condition','workerID'])['correct'].mean()).reset_index()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_confint(D.groupby(['orig_game_condition','rate_condition'])['correct'].agg(['mean','count','std']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
