{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Sketch Partitions for Rating Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the sketch paths from that folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_dir = os.path.abspath('../..')\n",
    "exp_name = 'classify_iternum'\n",
    "exp_dir = os.path.join(proj_dir,exp_name)\n",
    "sketch_dir = os.path.abspath(os.path.join(proj_dir,'sketches'))\n",
    "\n",
    "full_stim_paths = os.listdir(sketch_dir) # list out all the sketches in that directory\n",
    "sketches = [i for i in full_stim_paths if i.split('/')[-1] != '.DS_Store']\n",
    "\n",
    "stimListDir = os.path.abspath('../../experiments/classify_iternum/stimList')\n",
    "\n",
    "if not os.path.exists(stimListDir):\n",
    "    os.makedirs(stimListDir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble a dataframe from all the sketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_info = pd.DataFrame(columns = [\"orig_gameid\",\"orig_animal\",\"orig_cardinality\",\"orig_trial\",\"orig_cond\",\"orig_version\",\"sketch_url\"]) # initialize dataframe\n",
    "\n",
    "for i in range(len(sketches)): # for every sketch\n",
    "    name = sketches[i].split('_') # split up its metadata\n",
    "                                                        #    gameID         animal            cardinality             trialnum       condition      stim_version\n",
    "    stimurl = \"https://iternum-sketches.s3.amazonaws.com/\" + name[0] + '_' + name[1] + '_' + str(int(name[2])) + '_' + name[3] + '_' + name[4] + '_' + name[5]\n",
    "\n",
    "    # following two lines are dead:\n",
    "#     stimID = name[4].split('_') # ... by multiple delimiters\n",
    "#     stimurl = \"https://iternum-sketches.s3.amazonaws.com/\" + name[0] + '_' + stimID[0] + '_' + str(int(stimID[1])-1) + '_' + name[2] + '_' + name[3] + '_' + name[4]     \n",
    "    row = np.array([name[0],name[1],str(int(name[2])),name[3],name[4],name[5].split('.')[0],stimurl]) # put into relevant column\n",
    "    sketch_info.loc[len(sketch_info)] = row # now append that to the sketch info dataframe\n",
    "    \n",
    "sketch_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_info.iloc[0]['sketch_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make a small list of catch trials, referencing their location on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_nums = ['https://iternum-recog-catches.s3.amazonaws.com/1.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/2.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/3.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/4.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/5.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/6.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/7.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/8.png']\n",
    "\n",
    "catch_amls = ['https://iternum-recog-catches.s3.amazonaws.com/bear.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/deer.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/owl.png',\n",
    "              'https://iternum-recog-catches.s3.amazonaws.com/rabbit.png']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assemble the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = sketch_info    # the bag of sketches to sample, because sampling without replacement\n",
    "\n",
    "games = bag.orig_gameid.unique() # we want this to be a unique list of all the games\n",
    "paradigms = [] # this will be a list of dataframes, each dataframe containing the sketches to be rated by a rater\n",
    "\n",
    "\n",
    "batch = 0\n",
    "while len(bag) > 0: # sample from the bag without replacement\n",
    "    \n",
    "    # initialize paradigm \n",
    "    number_paradigm = {'versionID':batch,        # which partition is it (set of sketches) ?\n",
    "                       'classify_condition':'number',  # what feature will mturk recognizers be asked to classify?\n",
    "                       'games':[],               # empty list to be filled with classification games as they happen    \n",
    "                       'meta':[]}                # the whole [unordered] trial list goes in this 'meta' structure\n",
    "    shape_paradigm = {'versionID':batch+1,        # which partition is it (set of sketches) ?\n",
    "                       'classify_condition':'shape',  # what feature will mturk recognizers be asked to classify?\n",
    "                       'games':[],               # empty list to be filled with classification games as they happen    \n",
    "                       'meta':[]}                # the whole [unordered] trial list goes in this 'meta' structure\n",
    "\n",
    "    number_catches = catch_nums\n",
    "    animal_catches = catch_amls\n",
    "    random_seed = 619\n",
    "    for i in range(len(games)): # we want each rater to see [no more than] one sketch from each game\n",
    "        trial = {} # initialize a dictionary for this rater, 1 game per trial\n",
    "        \n",
    "        row = bag[bag['orig_gameid']==games[i]].sample(n=1,replace=False,random_state=random_seed) # sample a sketch at random from the game\n",
    "        bag = bag.drop(index = row.index) # remove it from the bag\n",
    "        \n",
    "        trial[\"orig_gameid\"] = row.iloc[0][\"orig_gameid\"]\n",
    "        trial[\"orig_animal\"] = row.iloc[0][\"orig_animal\"]\n",
    "        trial[\"orig_cardinality\"] = row.iloc[0][\"orig_cardinality\"]\n",
    "        trial[\"orig_trial\"] = row.iloc[0][\"orig_trial\"]\n",
    "        trial[\"orig_cond\"] = row.iloc[0][\"orig_cond\"]\n",
    "        trial[\"orig_version\"] = row.iloc[0][\"orig_version\"]\n",
    "        trial[\"sketch_url\"] = row.iloc[0][\"sketch_url\"]\n",
    "        trial[\"catchTrial\"] = False\n",
    "        \n",
    "        \n",
    "        number_paradigm['meta'].append(trial)\n",
    "        shape_paradigm['meta'].append(trial)\n",
    "        \n",
    "        # insert catch trials at regular intervals\n",
    "        if i in [0,1,2,3]:\n",
    "            np.random.seed(seed=random_seed + len(bag) + i)\n",
    "            catch_aml_trial = {}\n",
    "            catch_num_trial = {}\n",
    "            num_stim = np.random.choice(number_catches,replace=False)\n",
    "            aml_stim = np.random.choice(animal_catches,replace=False)\n",
    "            \n",
    "            \n",
    "            # first do the number one\n",
    "            catch_num_trial[\"orig_gameid\"] = np.nan\n",
    "            catch_num_trial[\"orig_animal\"] = np.nan\n",
    "            catch_num_trial[\"orig_cardinality\"] = num_stim.split('/')[-1].split('.')[0]\n",
    "            catch_num_trial[\"orig_trial\"] = np.nan\n",
    "            catch_num_trial[\"orig_cond\"] = np.nan\n",
    "            catch_num_trial[\"orig_version\"] = np.nan\n",
    "            catch_num_trial[\"sketch_url\"] = num_stim\n",
    "            catch_num_trial[\"catchTrial\"] = True\n",
    "            number_paradigm['meta'].append(catch_num_trial)\n",
    "            \n",
    "            # then do the animal one\n",
    "            catch_aml_trial[\"orig_gameid\"] = np.nan\n",
    "            catch_aml_trial[\"orig_animal\"] = aml_stim.split('/')[-1].split('.')[0]\n",
    "            catch_aml_trial[\"orig_cardinality\"] = np.nan\n",
    "            catch_aml_trial[\"orig_trial\"] = np.nan\n",
    "            catch_aml_trial[\"orig_cond\"] = np.nan\n",
    "            catch_aml_trial[\"orig_version\"] = np.nan\n",
    "            catch_aml_trial[\"sketch_url\"] = aml_stim\n",
    "            catch_aml_trial[\"catchTrial\"] = True\n",
    "            shape_paradigm['meta'].append(catch_aml_trial)\n",
    "            \n",
    "        \n",
    "        random_seed += 1 # this means that each paradigm is matched in its sequence for number and shape recognizers\n",
    "    \n",
    "    # when a paradigm is assembled, put it into the list:\n",
    "    paradigms.append(number_paradigm) # first put the version for people classifying the number info     \n",
    "    paradigms.append(shape_paradigm) # then store the exact same data structure but change the classification goal\n",
    "    \n",
    "    batch += 2\n",
    "    \n",
    "num_partitions = len(paradigms)    \n",
    "print('We have {} unique partitions.'.format(num_partitions)) # Should be 32*2=64 paradigms of 61 sketches; each rater sees one per game, requiring 64 raters        \n",
    "\n",
    "# print(paradigms[1].iloc[3,6])    # print one of the urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in paradigms:\n",
    "    print(i['versionID'], i['classify_condition'],i['meta'][5]['orig_animal'],i['meta'][5]['orig_cardinality'])\n",
    "\n",
    "# for thing in paradigms[0]['meta']:\n",
    "#     print(thing,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put that datastructure into Mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "import pymongo as pm\n",
    "import socket\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27017') \n",
    "db = conn['stimuli']\n",
    "coll = db['iternum_classification']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now really insert data\n",
    "# reallyDelete = False\n",
    "reallyRun = False\n",
    "\n",
    "## sometimes during beta testing we want to delete what's already in the collection\n",
    "# if reallyDelete:\n",
    "#     coll.delete_many({})\n",
    "\n",
    "## insert the data\n",
    "if reallyRun:\n",
    "    for (i,j) in enumerate(paradigms):\n",
    "        print ('%d of %d uploaded ...' % (i+1,len(paradigms)))\n",
    "        clear_output(wait=True)\n",
    "        coll.insert_one(j)\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra notes and things for remembering syntax:\n",
    "# coll.find_one()\n",
    "# len(list(coll.find({})))\n",
    "# print(db.command(\"collstats\", 'iternum_classification'))\n",
    "# list(coll.find({'games':'8067-374f2239-6bb9-4349-9786-afcfdc2ceb0c'}))[0]['versionID']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch for data lacunae (incomplete or invalid games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure out which partitions need to be done, because we forgot to store versionID in the recog data\n",
    "# get the list of valid games that form the complement to the set of games we still need to recruit\n",
    "validGamesList = np.load(\"../../analysis/datastructures/valid_game_ids.npy\",allow_pickle=True)\n",
    "\n",
    "# find out which games in each partition are actually valid and store them in a dictionary\n",
    "partitions_with_games = {}\n",
    "for partition in list(patch_coll.find({})): # for 1st patch, we looked at coll. For 2nd patch, we look at patch_coll\n",
    "    partitions_with_games[partition['versionID']] = list(set(partition['games']).intersection(validGamesList)) #partition['games']\n",
    "\n",
    "# copy the paradigms defined above for the original task\n",
    "patch_paradigms = paradigms\n",
    "\n",
    "# and then populate the copy with only the valid games that don't need to be done all over again\n",
    "for patch_paradigm in patch_paradigms:\n",
    "    patch_paradigm['games'] = partitions_with_games[patch_paradigm['versionID']]\n",
    "# the above two lines break when you put in 'iternum_classification_patching3';\n",
    "# probably because there is only one partition\n",
    "\n",
    "# check to see that it looks right:\n",
    "# for thing in patch_paradigms:\n",
    "#     print(len(thing['games']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vars \n",
    "auth = pd.read_csv('auth.txt', header = None) # this auth.txt file contains the password for the sketchloop user\n",
    "pswd = auth.values[0][0]\n",
    "user = 'sketchloop'\n",
    "host = 'cogtoolslab.org'\n",
    "\n",
    "import pymongo as pm\n",
    "import socket\n",
    "conn = pm.MongoClient('mongodb://sketchloop:' + pswd + '@127.0.0.1:27017') \n",
    "db = conn['stimuli']\n",
    "patch_coll = db['iternum_classification_patching3']\n",
    "\n",
    "# previously did 'iternum_classification_patching', but still had some pesky extras so needed to even it out again\n",
    "# after doing 'iternum_classification_patching2', there was ONE partition that needed ONE recog still\n",
    "# made 'iternum_classification_patching3' just to get one more valid game for 63rd partition ('versionID' : 62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now really insert data\n",
    "# reallyDelete = False\n",
    "reallyRun = False\n",
    "\n",
    "## sometimes during beta testing we want to delete what's already in the collection\n",
    "# if reallyDelete:\n",
    "#     patch_coll2.delete_many({})\n",
    "\n",
    "# insert the data\n",
    "if reallyRun:\n",
    "    for (i,j) in enumerate(patch_paradigms):\n",
    "        if i == 62:\n",
    "            print ('%d of %d uploaded ...' % (i+1,len(patch_paradigms)))\n",
    "            clear_output(wait=True)\n",
    "            patch_coll.insert_one(j)\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to make sure it worked\n",
    "patch_coll.find_one({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for thing in patch_coll.find({}):\n",
    "    print(len(thing['games']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
